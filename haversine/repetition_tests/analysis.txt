--------------------------------------------------
My desktop cpu: AMD Ryzen 9 5900X, 12-Core (24 logical processors), 4251 Mhz base clock
Family: 19h (Zen 3)
Guide: https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/software-optimization-guides/56665.zip
--------------------------------------------------

REPETITION TESTER
Compiled with a release profile to elide bounds/overflow checking wherever possible.
Optimized for binary size, loop vectorization disabled (-C opt-level="z")

loop in write_to_all_bytes

// ...
    let test_section = TimeTestSection::begin();
    for (index, element) in buffer.iter_mut().enumerate() {
        *element = index as u8;
    }
    test_section.end(buffer.len() as u64)
// ...

LOOP ASSEMBLY (EXCLUDING TEST SECTION START + END)
00007FF6115A2572  49 39 c6      cmp     r14, rax ; r14 must be loaded with the size of the buffer
00007FF6115A2575  74 08         jz      0x7ff6115a257f <repetition_testing::write_to_all_bytes+0x37>
00007FF6115A2577  88 04 03      mov     byte ptr [rbx+rax*1], al
00007FF6115A257A  48 ff c0      inc     rax
00007FF6115A257D  eb f3         jmp     0x7ff6115a2572 <repetition_testing::write_to_all_bytes+0x2a>

Total bytes CPU needs to process loop: 0x00007FF6115A257D - 0x00007FF6115A2572 = 11 bytes
Best reported throughput: 3.8630gb/s -> ~3.86gb/s
My CPU frequency: 4.25 GHz
(4.25 * 1000^3) / (3.86 * 1024^3) ~=  1.025 cycles per loop iteration
So, 11 bytes in ~1.025 cycles


--------------------------------------------------
ANALYSIS OF WRITE ALL BYTES FUNCTIONS (NORMAL + ASM)

write all bytes rust function
best: 3.9060gb/s
1.0133 cycles per iteration

write all bytes asm function
best: 3.9050gb/s
1.0136 cycles per iteration

write all bytes asm function, mov replaced with 3-byte nop
best: 3.9544gb/s
1.0010 cycles per iteration

write all bytes asm function, mov removed entirely
best: 3.9531gb/s
1.0013 cycles per iteration

write all bytes asm function, just decrementing
best: 3.9396gb/s
1.0046 cycles per iteration

Conclusion:
Observations match Casey's findings from the "Linking Directly to ASM for Experimentation" video.
Did originally encounter some weird behavior where one of the above functions would randomly drop to
a throughput of about 1.9gb/s. For some reason forcing the TimeTestSection functions to be
#[inline(never)] fixed that issue. TimeTestSection::begin and TimeTestSection::end were previously
set to #[inline(always)], so maybe that was causing the compiler to output code in a weird manner in
some cases.

Basically, there's something about that mov instruction that's slowing us down...

UPDATE 2024-09-27:
The weird behavior of some asm functions randomly dropping to 1.9gb/s (basically halved perf from
the others) is because of code alignment. To account for this I've updated the assembly to align
loop starts to 64-byte boundaries so all following instructions fit within instruction cache.

UPDATE 2024-09-27:
I think the reason why the versions using MOV are just slightly slower than the others is due to how
many store execution units are available on a Zen 3 CPU. According to the optimization guide, the
load-store unit can handle only two memory store operations per cycle (or one if the store is 128-
or 256-bit). Comparatively, the integer execution unit has four units that can do general purpose integer
operations (add, sub, maybe shift, etc).
So I don't think that the MOV instruction is adding some additional dependency. Sure, it needs to
know what's in AL but it has that in the register file as soon as the INC of the previous iteration
is performed. I just think that due to the fewer number of execution units available to do the
memory move operation itself, the CPU is just slightly bottlenecked and can't chew threw as many of
those dependency chains concurrently (does this last sentence even make sense?).


--------------------------------------------------
NOP LOOP REP TESTS ANALYSIS
Compiled with dev profile using -C opt-level=z
For some reason using the release profile is giving weird and somewhat unexpected results even
though it's running the exact same assembly and measuring time in the exact same ways...
The only thing I noticed was that maybe the location in the binary of the test code, the timing
code, and the assembly code might have been different between the release and dev profiles...

UPDATE: 2024-09-01
Turns out this is probably something to do with the code alignment. Maybe some of the asm was
straddling the edge of a cache line somewhere so the loops were maybe taking extra cycles to run
because of cache misses.

====== nop 3x1 all bytes ======
Min: 268574330 (63.1923ms) 3.9562gb/s 0pf
Max: 292086288 (68.7243ms) 3.6377gb/s 0pf
Avg: 270574940 (63.6630ms) 3.9269gb/s 0pf

====== nop 1x3 all bytes ======
Min: 268599065 (63.1981ms) 3.9558gb/s 0pf
Max: 315568771 (74.2495ms) 3.3670gb/s 0pf
Avg: 272565674 (64.1314ms) 3.8982gb/s 0pf

====== nop 3x3 all bytes ======
Min: 270788240 (63.7132ms) 3.9238gb/s 0pf
Max: 308137943 (72.5011ms) 3.4482gb/s 0pf
Avg: 272479330 (64.1111ms) 3.8995gb/s 0pf

====== nop 1x9 all bytes ======
Min: 537352915 (126.4326ms) 1.9773gb/s 0pf
Max: 590508217 (138.9394ms) 1.7993gb/s 0pf
Avg: 546492330 (128.5830ms) 1.9443gb/s 0pf

====== nop 5x3 all bytes ======
Min: 360979317 (84.9340ms) 2.9435gb/s 0pf
Max: 403110248 (94.8469ms) 2.6358gb/s 0pf
Avg: 366563157 (86.2478ms) 2.8986gb/s 0pf

====== nop 1x15 all bytes ======
Min: 805970425 (189.6350ms) 1.3183gb/s 0pf
Max: 860760108 (202.5263ms) 1.2344gb/s 0pf
Avg: 811325180 (190.8949ms) 1.3096gb/s 0pf

So the front end of the CPU suddenly begins to tank after about 3 3-byte nops...
For completeness, the results of the release test are included here.
This test was compiled with opt-level=z as well.

====== nop 3x1 all bytes ======
Min: 268607140 (63.2000ms) 3.9557gb/s 0pf
Max: 308021384 (72.4737ms) 3.4495gb/s 0pf
Avg: 270548212 (63.6567ms) 3.9273gb/s 0pf

====== nop 1x3 all bytes ======
Min: 537237527 (126.4054ms) 1.9778gb/s 0pf
Max: 557909570 (131.2693ms) 1.9045gb/s 0pf
Avg: 541612930 (127.4349ms) 1.9618gb/s 0pf

====== nop 3x3 all bytes ======
Min: 268814880 (63.2489ms) 3.9526gb/s 0pf
Max: 340899555 (80.2095ms) 3.1168gb/s 0pf
Avg: 273065053 (64.2489ms) 3.8911gb/s 0pf

====== nop 1x9 all bytes ======
Min: 537225755 (126.4027ms) 1.9778gb/s 0pf
Max: 589634314 (138.7338ms) 1.8020gb/s 0pf
Avg: 542055239 (127.5390ms) 1.9602gb/s 0pf

====== nop 5x3 all bytes ======
Min: 537344840 (126.4307ms) 1.9774gb/s 0pf
Max: 549804078 (129.3622ms) 1.9326gb/s 0pf
Avg: 540604533 (127.1976ms) 1.9654gb/s 0pf

====== nop 1x15 all bytes ======
Min: 805960012 (189.6325ms) 1.3183gb/s 0pf
Max: 878984854 (206.8144ms) 1.2088gb/s 0pf
Avg: 813267162 (191.3518ms) 1.3065gb/s 0pf

--------------------------------------------------
BRANCH PREDICTION TESTS

====== branch never ======
Min: 271847383 (63.9625ms) 3.9085gb/s 0pf
Max: 277948852 (65.3981ms) 3.8227gb/s 0pf
Avg: 273610915 (64.3774ms) 3.8833gb/s 0pf

====== branch always ======
Min: 538339638 (126.6650ms) 1.9737gb/s 0pf
Max: 580577540 (136.6030ms) 1.8301gb/s 0pf
Avg: 544086416 (128.0171ms) 1.9529gb/s 0pf

====== branch every 2 ======
Min: 404965372 (95.2836ms) 2.6237gb/s 0pf
Max: 433574885 (102.0151ms) 2.4506gb/s 0pf
Avg: 412483060 (97.0524ms) 2.5759gb/s 0pf

====== branch every 3 ======
Min: 525751903 (123.7032ms) 2.0210gb/s 0pf
Max: 1010188535 (237.6854ms) 1.0518gb/s 0pf
Avg: 839721044 (197.5765ms) 1.2653gb/s 0pf

====== branch every 4 ======
Min: 337020580 (79.2970ms) 3.1527gb/s 0pf
Max: 352578513 (82.9576ms) 3.0136gb/s 0pf
Avg: 342325217 (80.5451ms) 3.1039gb/s 0pf

====== branch rust rand ======
Min: 3310016408 (778.8078ms) 0.3210gb/s 0pf
Max: 3354717737 (789.3255ms) 0.3167gb/s 0pf
Avg: 3326073578 (782.5859ms) 0.3195gb/s 0pf

====== branch bcrypt rand ======
Min: 3310146670 (778.8384ms) 0.3210gb/s 0pf
Max: 3352123793 (788.7152ms) 0.3170gb/s 0pf
Avg: 3319194412 (780.9673ms) 0.3201gb/s 0pf

--------------------------------------------------
CODE ALIGNMENT TESTS

====== loop_aligned_64 ======
Min: 16777257 (3.9475ms) 3.9582gb/s 0pf
Max: 17378547 (4.0890ms) 3.8213gb/s 0pf
Avg: 16964324 (3.9915ms) 3.9146gb/s 0pf

====== loop_aligned_1 ======
Min: 16777215 (3.9475ms) 3.9582gb/s 0pf
Max: 17915407 (4.2153ms) 3.7068gb/s 0pf
Avg: 16840081 (3.9623ms) 3.9435gb/s 0pf

====== loop_aligned_15 ======
Min: 16777215 (3.9475ms) 3.9582gb/s 0pf
Max: 17372385 (4.0875ms) 3.8226gb/s 0pf
Avg: 16886883 (3.9733ms) 3.9325gb/s 0pf

====== loop_aligned_31 ======
Min: 16777215 (3.9475ms) 3.9582gb/s 0pf
Max: 36698282 (8.6346ms) 1.8096gb/s 0pf
Avg: 16882190 (3.9722ms) 3.9336gb/s 0pf

====== loop_aligned_63 ======
Min: 33554430 (7.8949ms) 1.9791gb/s 0pf
Max: 34336302 (8.0789ms) 1.9341gb/s 0pf
Avg: 33846440 (7.9636ms) 1.9620gb/s 0pf

--------------------------------------------------
PROBING READ EXECUTION PORTS

Results:

====== 1 read per iteration ======
Min: 4194282 (0.9869ms) 3.9582gb/s 0pf
Max: 8830183 (2.0776ms) 1.8801gb/s 0pf
Avg: 4205617 (0.9895ms) 3.9476gb/s 0pf

====== 2 reads per iteration ======
Min: 2097162 (0.4934ms) 7.9164gb/s 0pf
Max: 2745925 (0.6461ms) 6.0460gb/s 0pf
Avg: 2120079 (0.4988ms) 7.8308gb/s 0pf

====== 3 reads per iteration ======
Min: 1398122 (0.3290ms) 11.8745gb/s 0pf
Max: 1896775 (0.4463ms) 8.7527gb/s 0pf
Avg: 1404620 (0.3305ms) 11.8196gb/s 0pf

====== 4 reads per iteration ======
Min: 1398080 (0.3290ms) 11.8749gb/s 0pf
Max: 1874335 (0.4410ms) 8.8575gb/s 0pf
Avg: 1403615 (0.3303ms) 11.8280gb/s 0pf

So it seems like my processor (zen 3 amd) has at least 3 execution ports it can utilize.
The manual (see "2.12 Load-Store Unit" in the amd family 19h software optimization guide) seems to confirm this:
    "The load-store (LS) unit ... contains three largely independent pipelines enabling the execution of three 256-bit memory operations per cycle."

--------------------------------------------------
PROBING WRITE EXECUTION PORTS

Results:

====== 1 write per iteration ======
Min: 4194325 (0.9869ms) 3.9582gb/s 0pf
Max: 8475287 (1.9941ms) 1.9589gb/s 0pf
Avg: 4252477 (1.0006ms) 3.9041gb/s 0pf

====== 2 writes per iteration ======
Min: 2097162 (0.4934ms) 7.9164gb/s 0pf
Max: 4607383 (1.0841ms) 3.6033gb/s 0pf
Avg: 2114576 (0.4975ms) 7.8512gb/s 0pf

====== 3 writes per iteration ======
Min: 2097162 (0.4934ms) 7.9164gb/s 0pf
Max: 3547432 (0.8347ms) 4.6800gb/s 0pf
Avg: 2106004 (0.4955ms) 7.8832gb/s 0pf

====== 4 writes per iteration ======
Min: 2097162 (0.4934ms) 7.9164gb/s 0pf
Max: 2728075 (0.6419ms) 6.0856gb/s 0pf
Avg: 2111109 (0.4967ms) 7.8641gb/s 0pf

Observe that we really only get better best-case performance with 2 writes per iteration (though interestingly our worst case keeps getting better...).
This makes sense because according to the amd family 19h software optimization guide section 2.12 "Load-Store Unit", "A maximum of two of the [three] memory
operations can be stores, with a maximum of one store if the store is a 128- or 256-bit store."

--------------------------------------------------
READ WIDTH TESTS

Notice how each time we double the number of bytes we read per instruction, we basically double our
performance.

See notes in the src/read_widths.asm file of the test.

Results:

====== 3 4-byte (32-bit) reads ======
Min: 1398122 (0.3290ms) 47.4980gb/s 0pf
Max: 2092487 (0.4923ms) 31.7364gb/s 0pf
Avg: 1411420 (0.3321ms) 47.0505gb/s 0pf

====== 3 8-byte (64-bit) reads ======
Min: 699040 (0.1645ms) 94.9988gb/s 0pf
Max: 1019745 (0.2399ms) 65.1222gb/s 0pf
Avg: 703955 (0.1656ms) 94.3356gb/s 0pf

====== 3 16-byte (128-bit) reads ======
Min: 524280 (0.1234ms) 126.6651gb/s 0pf
Max: 784890 (0.1847ms) 84.6080gb/s 0pf
Avg: 524938 (0.1235ms) 126.5064gb/s 0pf

====== 3 32-byte (256-bit) reads ======
Min: 262267 (0.0617ms) 253.2076gb/s 0pf
Max: 532695 (0.1253ms) 124.6642gb/s 0pf
Avg: 268421 (0.0632ms) 247.4024gb/s 0pf

--------------------------------------------------
POWER OF 2 CACHE SIZE TESTS

So windows reports the following CPU cache sizes:
L1: 768kb
L2: 6mb
L3: 64mb

From https://www.techpowerup.com/cpu-specs/ryzen-9-5900x.c2363:
L1: 64kb (per core)
L2: 512kb (per core)
L3: 64mb

SIZE    THROUGHPUT(GB/S)
4kb	    253.1921
8kb	    249.7821
16kb	253.1844
24kb	246.2609
30kb	247.6308
31kb	247.8063
32kb	252.5144
40kb	127.3201
48kb	126.7835
64kb	126.6634
128kb	126.6615
256kb	126.6615
300kb	126.6604
350kb	126.6332
400kb	126.658
450kb	126.5103
512kb	118.2206
1mb	    104.6816
2mb	    104.7321
4mb	    104.7206
6mb	    104.2203
8mb	    103.553
12mb	105.0173
16mb	103.4035
24mb	93.0089
32mb	53.4944
40mb	45.4983
48mb	41.0435
56mb	37.6805
60mb	36.9164
62mb	37.0097
63mb	36.1865
64mb	35.4803
128mb	30.131
256mb	28.9991


Remember that L1 cache is usually split in half with separate sections for code and data. That would
explain why even though we have 64kb L1 per core we see a drop from 32kb to 64kb. We see a slight
drop from 256kb to 512kb even though we have 512kb per core because our code takes up part of the
cache, so we can't fit a whole 512kb sub-buffer into the 512kb cache.

I'm not sure why 24kb, 30kb, 31kb are slightly slower (by an almost insignificant amount actually)
than 32kb. I will revisit this in the future. Maybe something is wrong with my non-power-of-two
assembly routine or with the number of bytes I'm reporting as having been processed...

I'm also not sure why I'm seeing a consistent drop in throughput in the L3 from 512kb+ to 64mb.
Maybe it's that L3 is shared among all cores on the CPU so there are more cache misses because
processes running on other cores are also using it.

--------------------------------------------------
UNALIGNED LOAD PENALTIES

Table entries are measured in gb/s

offset	16k	(L1)    128k (L2)	32mb (L3)   256mb (RAM)
0       242.8874	126.6041	56.2896	    29.9816
1	    131.5153	126.2359	48.1305	    29.9503
2	    131.5658	126.2258	47.5226	    29.5751
4	    131.6701	126.2182	48.2048	    29.9928
8	    131.5998	126.1271	47.492	    30.0092
16	    131.5004	126.2289	47.6962	    29.966
24	    131.6706	126.1938	47.5907	    29.9898
32	    242.8874	126.4146	55.6822	    30.5529
48	    132.0802	126.1359	44.2921	    29.6253
56	    132.101	    126.1491	44.5958	    29.5113
64	    242.8874	126.5344	52.5317	    29.6326

L1:
As expected, forcing the CPU to load 2 cache lines into 2 load buffers and stitch them together to
get a result halves our throughput. Also note here that the assembly uses ymm registers which are 32
bytes wide, so with an offset of 32 bytes we're still perfectly aligned and can load using exactly 1
cache line.

L2:
Interestingly, no change in behavior regardless of change in base pointer offset.

L3:
Very slight changes in behavior depending on base pointer offset.

RAM:
No changes really.

--------------------------------------------------
NONTEMPORAL STORES

Processing 1073741824 worth of reads/writes
INPUT/OUTPUT BUFFER SIZE: 67108864
L1 DATA CACHE SIZE: 16384
L2 CACHE SIZE: 524288
L3 CACHE SIZE: 67108864
cpu frequency estimate: 4250118618

====== temporal stores ======
Min: 385722393 (90.7557ms) 11.0186gb/s 0pf
Max: 482286897 (113.4761ms) 8.8124gb/s 0pf
Avg: 395868318 (93.1429ms) 10.7362gb/s 0pf

====== nontemporal stores ======
Min: 237659023 (55.9182ms) 17.8833gb/s 0pf
Max: 325592968 (76.6080ms) 13.0535gb/s 0pf
Avg: 244321906 (57.4859ms) 17.3956gb/s 0pf

So bypassing the cache hierarchy allows us to do writes without having to worry about needlessly evicting our input buffer from our caches.
